{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "336d771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "\n",
    "from fastai.data.all import Transform, DataBlock, RandomSplitter, L\n",
    "from fastai.vision.all import *\n",
    "from fastai.distributed import *\n",
    "from fastai.callback.all import SaveModelCallback, EarlyStoppingCallback\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# Load StarCraft2Sensor stuff\n",
    "ipynb_dir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "code_root = os.path.join(ipynb_dir, '..')\n",
    "sys.path.append(code_root)  # Needed for import below\n",
    "\n",
    "from sc2sensor.dataset import StarCraftSensor\n",
    "from sc2sensor.utils.sensor_utils import SensorPlacementDataset, SUPPORTED_PLACEMENT_KINDS\n",
    "from sc2sensor.utils.unit_type_data import NONNEUTRAL_CHANNEL_TO_ID, NONNEUTRAL_ID_TO_NAME\n",
    "CHANNEL_TO_NAME = [NONNEUTRAL_ID_TO_NAME[NONNEUTRAL_CHANNEL_TO_ID[i]] for i in range(len(NONNEUTRAL_CHANNEL_TO_ID))]\n",
    "# removing barrier_h and barrier_v from the placement kinds since diag kinds are more fitting\n",
    "PLACEMENT_KINDS = [kind for kind in SUPPORTED_PLACEMENT_KINDS if not (kind.endswith('_h') or kind.endswith('_v'))]\n",
    "\n",
    "\n",
    "data_root = os.path.join(code_root, 'data') # Data root directory\n",
    "data_subdir = 'starcraft-sensor-dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49641c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_model_root = Path('/local/scratch/a/skulinsk/starcraft-dataset-extraction-private/models/extracted_models')\n",
    "# benchmark_results_save_root = Path('/local/scratch/a/skulinsk/starcraft-dataset-extraction-private/paper-demos/benchmark-results')\n",
    "\n",
    "extracted_model_root = Path('/local/scratch/a/shared/starcraft_shared/benchmarks/benchmark-models')\n",
    "benchmark_results_save_root = Path('/local/scratch/a/shared/starcraft_shared/benchmarks/v2-benchmark-results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4a6b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $benchmark_results_save_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b23b64f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm $benchmark_results_save_root/*.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2023649b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with inputs: {'max_samples': 10000, 'batch_size': 24, 'n_epochs': 10, 'max_models': 10000}\n",
      "Using a max number of a 10000\n"
     ]
    }
   ],
   "source": [
    "class FakeArgs():\n",
    "    def __str__(self):\n",
    "        return str(self.__dict__)\n",
    "\n",
    "args = FakeArgs()\n",
    "args.max_samples = 10000\n",
    "args.batch_size = 24\n",
    "args.n_epochs = 10\n",
    "args.max_models = 10000 # Set to 1 if you want to do a quick test run\n",
    "\n",
    "print(f'Starting with inputs: {str(args)}')\n",
    "\n",
    "if args.max_samples is not None:\n",
    "    print(f'Using a max number of a {args.max_samples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51430acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_benchmark_results(save_root, result, name, benchmark_kind,\n",
    "                             sensor_kwargs=None, metric_name='MSE', raw_model_path=None):\n",
    "    if benchmark_kind == 'unit_identification':\n",
    "        results_file = 'unit_identification_results.csv' if sensor_kwargs is None else 'sensor_unit_identification_results.csv'\n",
    "    elif benchmark_kind == 'next_window':\n",
    "        results_file = 'next_window_results.csv' if sensor_kwargs is None else 'sensor_next_window_results.csv'\n",
    "    else:\n",
    "        raise NotImplementedError(f'No benchmark file exists for type {benchmark_kind}')\n",
    "    print(f'Appending to results file: {results_file}')\n",
    "        \n",
    "    save_root = Path(save_root)\n",
    "    if (save_root/results_file).exists():\n",
    "        current_results = pd.read_csv(save_root/results_file)\n",
    "    else:\n",
    "        current_results = pd.DataFrame()\n",
    "        \n",
    "    if sensor_kwargs is None:\n",
    "      sensor_kwargs = {'n_sensors': 1, 'radius': 1000, 'kind': 'oracle', 'failure_rate': 0}\n",
    "        \n",
    "    results_to_be_saved = {\n",
    "        'model_name':name,\n",
    "         #metric_name: [result],\n",
    "        # result is now a dictionary of metrics\n",
    "        **result,\n",
    "        'sensor_kind':sensor_kwargs['kind'],\n",
    "        **sensor_kwargs,\n",
    "        'sensor_kwargs': [sensor_kwargs],\n",
    "        'raw_model_path':str(raw_model_path) if isinstance(raw_model_path, Path) else raw_model_path}\n",
    "    \n",
    "    \n",
    "    results_df = pd.concat((current_results, pd.DataFrame(results_to_be_saved)))\n",
    "    results_df.to_csv(save_root/results_file, index=False)\n",
    "    return True\n",
    "\n",
    "def get_best_model(model_root, with_suffix=False):\n",
    "    model_root = Path(model_root)\n",
    "    train_history = pd.read_csv(model_root/'train_history.csv')\n",
    "    best_epoch = train_history.valid_loss.argmin()\n",
    "    best_model = [i for i in list(model_root.glob('*')) if str(i).endswith(f'_{best_epoch}.pth')]\n",
    "    assert len(best_model) == 1, f'Found {len(best_model)} best model(s) instead of just 1.'\n",
    "    if with_suffix:\n",
    "        return best_model[0]\n",
    "    else:\n",
    "        return best_model[0].with_suffix('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6f4053",
   "metadata": {},
   "source": [
    "# Next Window Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "659e73ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subclass of original dataset for next window prediction\n",
    "class NextWindowDataset(StarCraftSensor):\n",
    "  def __init__(self, *args, max_samples=None, **kwargs):\n",
    "    assert 'use_sparse' not in kwargs, 'Cannot set use_sparse with this dataset.'\n",
    "    assert 'compute_labels' not in kwargs, 'Cannot set use_sparse with this dataset.'\n",
    "    super().__init__(*args, use_sparse=True, compute_labels=False, **kwargs)\n",
    "    self.max_samples = max_samples\n",
    "    \n",
    "    # Sort data so that next index is merely + 1\n",
    "    self.metadata = self.metadata.sort_values(['static.replay_name', 'dynamic.window_idx']).reset_index(drop=True)\n",
    "    md = self.metadata\n",
    "    \n",
    "    # Get starting window indices\n",
    "    start_windows = md[(md['dynamic.num_windows'] > 1) \n",
    "                       & (md['dynamic.window_idx'] < (md['dynamic.num_windows'] - 1))]\n",
    "    self.start_idx = start_windows.index\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # Get original indices of start and end based on input \n",
    "    md = self.metadata\n",
    "    # Assumes sorted\n",
    "    orig_idx = self.start_idx[idx]\n",
    "    next_idx = orig_idx + 1\n",
    "    # Only sanity check first and last as this may be expensive\n",
    "    if idx == 0 or idx == len(self) - 1:\n",
    "      assert md['static.replay_name'][orig_idx] == md['static.replay_name'][next_idx], 'Replays are not the same'\n",
    "      assert md['dynamic.window_idx'][orig_idx] + 1 == md['dynamic.window_idx'][next_idx], 'Window indices are not adjacent'\n",
    "\n",
    "    # Get combined hyperspectral images\n",
    "    def get_hyperspectral_dense(idx):\n",
    "      # Concatenate player1 and player2 hyperspectral\n",
    "      replay_file, window_idx = self._get_replay_and_window_idx(idx)\n",
    "      with np.load(replay_file) as data:\n",
    "        player_1_hyperspectral = self._extract_hyperspectral(\n",
    "          'player_1', data, window_idx)\n",
    "        player_2_hyperspectral = self._extract_hyperspectral(\n",
    "          'player_2', data, window_idx)\n",
    "      return torch.concat([player_1_hyperspectral.to_dense(), \n",
    "                           player_2_hyperspectral.to_dense()], \n",
    "                          dim=-3).float()\n",
    "    windows = [get_hyperspectral_dense(idx) for idx in [orig_idx, next_idx]] \n",
    "    x = windows[0]\n",
    "    y = windows[1] - windows[0] # Compute diff\n",
    "    return x, y\n",
    "\n",
    "  def __len__(self):\n",
    "    if self.max_samples is not None:\n",
    "      return min(self.max_samples, len(self.start_idx))\n",
    "    else:\n",
    "      return len(self.start_idx)\n",
    "\n",
    "# Create fastai dataloaders given the PyTorch dataset\n",
    "class AddChannelCodes(Transform):\n",
    "  \"Add the code metadata to a `TensorMask`\"\n",
    "  def __init__(self, codes=None):\n",
    "      self.codes = codes\n",
    "      if codes is not None: self.vocab,self.c = codes,len(codes)\n",
    "\n",
    "  def decodes(self, o):\n",
    "      if self.codes is not None: o.codes=self.codes\n",
    "      return o\n",
    "    \n",
    "# HACK: Put all instances in both \"train\" and \"valid\"\n",
    "# From https://forums.fast.ai/t/solved-not-splitting-datablock/84759/3\n",
    "def all_splitter(o): return L(int(i) for i in range(len(o))), L(int(i) for i in range(len(o)))\n",
    "    \n",
    "SC2_CODES = [\n",
    "  f'{player}_{name}'\n",
    "  for player in ['P1','P2']\n",
    "  for name in CHANNEL_TO_NAME\n",
    "]\n",
    "def create_dataloaders_from_dataset(dataset, splitter=None, **kwargs):\n",
    "    # Needs to have reference to dataset for closures\n",
    "    assert 'get_x' not in kwargs\n",
    "    assert 'get_y' not in kwargs\n",
    "    assert 'get_items' not in kwargs\n",
    "    splitter = splitter if splitter is not None else RandomSplitter(seed=0)\n",
    "    assert len(SC2_CODES) == dataset[0][0].shape[0], 'Number of codes does not match number of channels'\n",
    "    block = DataBlock(\n",
    "        get_items=lambda d: list(range(len(d))),\n",
    "        get_x=lambda idx: dataset[idx][0],\n",
    "        get_y=lambda idx: dataset[idx][1],\n",
    "        blocks=None, # These are just transforms\n",
    "        splitter=splitter,\n",
    "        item_tfms=[AddChannelCodes(SC2_CODES)],\n",
    "    )\n",
    "    return block.dataloaders(dataset, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec9424f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached CSV metadata\n",
      "Not computing labels\n",
      "Post-processing metadata\n",
      "Finished dataset init\n",
      "Num Test: 10000\n",
      "x.shape: torch.Size([340, 64, 64]), y.shape: torch.Size([340, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Load test set\n",
    "next_window_test = NextWindowDataset(root=data_root, subdir=data_subdir,\n",
    "                                     train=False, max_samples=args.max_samples)\n",
    "# We should not clip the test set\n",
    "#, max_samples=args.max_samples)\n",
    "\n",
    "print(f'Num Test: {len(next_window_test)}')\n",
    "print(f'x.shape: {next_window_test[-1][0].shape}, y.shape: {next_window_test[-1][1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be92c2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MSELoss(), <__main__.BinnedNonZeroFalsePositiveRate object at 0x7f296ece5690>, <__main__.BinnedTruePositiveRate object at 0x7f296ece5710>, <__main__.BinnedTruePositiveRate object at 0x7f296ece57d0>]\n",
      "['MSE', 'FPR', 'TPR(1)', 'TPR(-1)']\n"
     ]
    }
   ],
   "source": [
    "# Create metrics\n",
    "from fastai.metrics import AvgMetric, Metric\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class NumWindowsMetric(Metric):\n",
    "  def reset(self):           self.count = 0\n",
    "  def accumulate(self, learn):\n",
    "    bs = find_bs(learn.yb)\n",
    "    #print(self.total)\n",
    "    self.count += bs\n",
    "  @property\n",
    "  def value(self): return self.count if self.count != 0 else None\n",
    "  @property\n",
    "  def name(self):  return 'NumWindows' \n",
    "\n",
    "# Example that takes an average (while considering batch size `bs`)\n",
    "# %% ../nbs/13a_learner.ipynb 119\n",
    "class BinnedConfusionMatrixMetric(Metric):\n",
    "  \"\"\"Micro confusion matrix (enables computation of micro-versions of other metrics)\n",
    "     (i.e., will give total percentages over all batches)\n",
    "  \"\"\"\n",
    "  def __init__(self):  self.labels = [-1.0, 1.0, 0.0]\n",
    "  def reset(self):           self.total,self.count = 0.0,0\n",
    "  def accumulate(self, learn):\n",
    "    bs = find_bs(learn.yb)\n",
    "    # Scales by batch size (later divided by batch size)\n",
    "    self.total += learn.to_detach(self._metric_batch(learn.pred, *learn.yb)) * bs\n",
    "    #print(self.total)\n",
    "    self.count += bs\n",
    "  def _metric_batch(self, y_pred, y_true):\n",
    "    # Computes confusion matrix for this batch (Normalized to percentages)\n",
    "    # First convert to 1, 0, or -1\n",
    "    def bin_data(y):\n",
    "      near_zero_mask = y.abs().lt(1/255.0)\n",
    "      y = torch.sign(y)\n",
    "      y[near_zero_mask] = 0\n",
    "      return y\n",
    "    y_pred, y_true = bin_data(y_pred), bin_data(y_true)\n",
    "    labels = self.labels\n",
    "    # Sklearn confusion_matrix is far too slow, vectorized version below\n",
    "    cm = np.zeros((len(labels), len(labels)))\n",
    "    for i, lab_i in enumerate(labels):\n",
    "      for j, lab_j in enumerate(labels):\n",
    "        cm[i, j] = torch.logical_and(y_true == lab_i, y_pred == lab_j).float().mean().cpu().numpy()\n",
    "    return cm\n",
    "\n",
    "  @property\n",
    "  def value(self): return self.total/self.count if self.count != 0 else None\n",
    "  @property\n",
    "  def name(self):  return 'BinnedConfusionMatrix' \n",
    "  \n",
    "class BinnedNonZeroFalsePositiveRate(BinnedConfusionMatrixMetric):\n",
    "  \"\"\"Micro FPR where 0 is negative - (True zeros but predicted nonzero)/(Num true zeros)\"\"\"\n",
    "  @property\n",
    "  def value(self):\n",
    "    print('FPR calculation')\n",
    "    micro_cm = self.total / self.count\n",
    "    zero_idx = self.labels.index(0.0)\n",
    "    correctly_predicted_zeros = micro_cm[zero_idx, zero_idx] # TN\n",
    "    total_true_zeros = micro_cm[zero_idx, :].sum() # N\n",
    "    TNR = correctly_predicted_zeros/total_true_zeros # TN/N\n",
    "    # FPR = 1 - TNR\n",
    "    return 1 - TNR if self.count != 0 else None\n",
    "  @property\n",
    "  def name(self):  return 'FPR' \n",
    "  \n",
    "class BinnedTruePositiveRate(BinnedConfusionMatrixMetric):\n",
    "  \"\"\"Micro FPR where 0 is negative - (True zeros but predicted nonzero)/(Num true zeros)\"\"\"\n",
    "  def __init__(self, positive_value):\n",
    "    super().__init__()\n",
    "    self.positive_value = positive_value\n",
    "  @property\n",
    "  def value(self):\n",
    "    micro_cm = self.total / self.count\n",
    "    positive_idx = self.labels.index(self.positive_value)\n",
    "    correctly_predicted = micro_cm[positive_idx, positive_idx] # TP\n",
    "    total_true = micro_cm[positive_idx, :].sum() # P\n",
    "    TPR = correctly_predicted/total_true # TP/P\n",
    "    return TPR if self.count != 0 else None\n",
    "  @property\n",
    "  def name(self):  return f'TPR({self.positive_value:.0f})'\n",
    "  \n",
    "next_window_metrics = [\n",
    "  nn.MSELoss(), \n",
    "  BinnedNonZeroFalsePositiveRate(), \n",
    "  BinnedTruePositiveRate(1.0), \n",
    "  BinnedTruePositiveRate(-1.0)\n",
    "]\n",
    "next_window_metric_names = [m.name if hasattr(m, 'name') else 'MSE' for m in next_window_metrics]\n",
    "print(next_window_metrics)\n",
    "print(next_window_metric_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a41ef611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_next_window_model(arch, batch_size, path_to_model,\n",
    "                           next_window_dataset, sensor_kwargs=None):\n",
    "\n",
    "    if sensor_kwargs is not None:\n",
    "        # build dataset with specific sensor type\n",
    "        assert all(('n_sensors' in sensor_kwargs,\n",
    "                   'radius' in sensor_kwargs,\n",
    "                   'kind' in sensor_kwargs,\n",
    "                   'failure_rate' in sensor_kwargs)) \n",
    "        next_window_dataset = SensorPlacementDataset(next_window_dataset, **sensor_kwargs,\n",
    "                                                     return_mask=False, make_cache=False,\n",
    "                                                     noiseless_ground_truth=True)\n",
    "        \n",
    "    dls = create_dataloaders_from_dataset(next_window_dataset, splitter=all_splitter, batch_size=batch_size)\n",
    "    print(f'Test dataset has {len(next_window_dataset)} windows and the valid dataloader has approximately {len(dls.valid)*batch_size} windows')\n",
    "\n",
    "    model = smp.Unet(\n",
    "        encoder_name=arch,        \n",
    "        encoder_weights='imagenet',     \n",
    "        in_channels=len(SC2_CODES),                  \n",
    "        classes=len(SC2_CODES),                      \n",
    "        activation=None, \n",
    "    )\n",
    "    \n",
    "    learner = Learner(model=model, dls=dls, loss_func=nn.MSELoss())\n",
    "    learner.metrics = next_window_metrics\n",
    "    learner = learner.load(path_to_model)\n",
    "    \n",
    "#     with learner.distrib_ctx():\n",
    "    result = learner.validate()\n",
    "    # Now that training has finished, empty the cache\n",
    "    print('Number of results vs number of names (I think original loss is automatically included)')\n",
    "    print(len(result), len(next_window_metric_names))\n",
    "    torch.cuda.empty_cache()\n",
    "    result_dict = {m: r for r, m in zip(result[1:], next_window_metric_names)}\n",
    "    print(result_dict)\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a5c85b",
   "metadata": {},
   "source": [
    "## Gathering next hyperspectral window results  (no sensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb0c533",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n",
      "Starting testing on next window dataset. [NOTE: Not using sensors] Testing with \n",
      "['resnet18', 'resnet34', 'resnet50', 'resnext50_32x4d']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ------------------Starting unet_resnet18.-------------------\n",
      "Using model /local/scratch/a/shared/starcraft_shared/benchmarks/benchmark-models/next_window/unet_resnet18/next_window_60000_unet_resnet18_7\n",
      "Test dataset has 10000 windows and the valid dataloader has approximately 10008 windows\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR calculation\n",
      "Number of results vs number of names (I think original loss is automatically included)\n",
      "5 4\n",
      "{'MSE': 3.85090708732605, 'FPR': 0.15030505040575948, 'TPR(1)': 0.6058672497798525, 'TPR(-1)': 0.5558459095472176}\n",
      "Appending to results file: next_window_results.csv\n",
      "Finished unet_resnet18placement.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ------------------Starting unet_resnet34.-------------------\n",
      "Using model /local/scratch/a/shared/starcraft_shared/benchmarks/benchmark-models/next_window/unet_resnet34/next_window_60000_unet_resnet34_8\n",
      "Test dataset has 10000 windows and the valid dataloader has approximately 10008 windows\n"
     ]
    }
   ],
   "source": [
    "%pdb on\n",
    "# Next window experiments  (not using sensors)\n",
    "next_window_model_root = extracted_model_root / 'next_window'\n",
    "architectures = (\n",
    "  'resnet18',\n",
    "  'resnet34',\n",
    "  'resnet50',\n",
    "  'resnext50_32x4d',\n",
    ")\n",
    "\n",
    "experiments = [*architectures]\n",
    "\n",
    "print(f'Starting testing on next window dataset. [NOTE: Not using sensors]',\n",
    "      f'Testing with \\n{experiments}')\n",
    "\n",
    "\n",
    "for arch in experiments[:min(args.max_models,len(experiments))]:\n",
    "    name = 'unet_' + arch\n",
    "    print('\\n'*5, f'Starting {name}.'.center(60, '-'))\n",
    "\n",
    "    path_to_model = get_best_model(next_window_model_root / name)\n",
    "    print(f'Using model {path_to_model}')\n",
    "    \n",
    "    result = test_next_window_model(arch, args.batch_size, path_to_model, next_window_test, None)\n",
    "    \n",
    "    save_benchmark_results(benchmark_results_save_root, result, name, benchmark_kind='next_window',\n",
    "                             sensor_kwargs=None, metric_name='MSE', raw_model_path=path_to_model)\n",
    "\n",
    "    print(f'Finished {name}placement.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb41e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(benchmark_results_save_root/'next_window_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac7945c",
   "metadata": {},
   "source": [
    "## Gathering *sensor* next hyperspectral window results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b825cc4f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Next window experiments  (not using sensors)\n",
    "sensor_next_window_model_root = extracted_model_root / 'sensor_next_window'\n",
    "\n",
    "sensor_next_window_model_dirs = [f for f in sensor_next_window_model_root.glob('**/*') \n",
    "                                   if f.is_dir() and not str(f).endswith('_sensors')]\n",
    "\n",
    "print(f'Starting testing on sensor next window dataset.')\n",
    "\n",
    "\n",
    "for model_dir in sensor_next_window_model_dirs[:min(args.max_models,len(sensor_next_window_model_dirs))]:\n",
    "    sensor_kind = model_dir.parent.name.split('_sensors')[0]\n",
    "    name = model_dir.name\n",
    "    arch = name.split('unet_')[-1]\n",
    "    \n",
    "    sensor_kwargs = {'n_sensors': 50,\n",
    "                     'radius': 5.5,\n",
    "                     'kind': sensor_kind,\n",
    "                     'failure_rate': 0.2}\n",
    "    \n",
    "    print('\\n'*5, f'Starting {name} with {sensor_kind} placement.'.center(60, '-'))\n",
    "\n",
    "    path_to_model = get_best_model(model_dir)\n",
    "    \n",
    "    print(f'Using model {path_to_model}')\n",
    "    \n",
    "    result = test_next_window_model(arch, args.batch_size, path_to_model, next_window_test, sensor_kwargs)\n",
    "    \n",
    "    save_benchmark_results(benchmark_results_save_root, result, name, benchmark_kind='next_window',\n",
    "                             sensor_kwargs=sensor_kwargs, metric_name='MSE', raw_model_path=path_to_model)\n",
    "\n",
    "    print(f'Finished {name} with {sensor_kind} placement.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21869c75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.read_csv(benchmark_results_save_root/'sensor_next_window_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32801352",
   "metadata": {},
   "source": [
    "# Unit Type Identification Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17edafb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sc2sensor.utils.unit_type_data import NONNEUTRAL_CHANNEL_TO_ID, NONNEUTRAL_ID_TO_NAME\n",
    "CHANNEL_TO_NAME = [NONNEUTRAL_ID_TO_NAME[NONNEUTRAL_CHANNEL_TO_ID[i]] for i in range(len(NONNEUTRAL_CHANNEL_TO_ID))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc491d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "class SegmentationDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, segment_path, create_metadata=True):\n",
    "        super().__init__()\n",
    "        self.path = Path(segment_path)\n",
    "        self.X_filenames = self._get_files()\n",
    "        if create_metadata:\n",
    "            self.metadata, self.match_metadata = self._make_metadata()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X_filename = self.X_filenames[idx]\n",
    "        y_filename = os.path.splitext(X_filename)[0].replace('images','labels') + '_labels.png'\n",
    "        \n",
    "#         return torch_read_image(str(X_filename)), torch_read_image(str(y_filename)).squeeze()\n",
    "        return (plt.imread(str(X_filename))*255).astype(np.uint8), \\\n",
    "               (plt.imread(str(y_filename))*255).astype(np.uint8)\n",
    "\n",
    "    def _get_files(self):\n",
    "        files =  list((self.path / 'images').glob('**/*.png'))\n",
    "        assert len(files) > 0, f'No .png files found in {self.path}'\n",
    "        return files\n",
    "    \n",
    "    def _make_metadata(self):\n",
    "        replay_names = [str(f).split('_')[-2].split('/')[-1] for f in self.X_filenames]\n",
    "        metadata = pd.DataFrame({'static.replay_name':replay_names})\n",
    "        match_metadata = metadata.drop_duplicates(subset=['static.replay_name']).reset_index(drop=True)\n",
    "        return metadata, match_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c3bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_data_path = Path(data_root)/'starcraft-sensor-dataset'/'segment'/'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b88fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics\n",
    "from fastai.metrics import AvgMetric, Metric\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class NumWindowsMetric(Metric):\n",
    "  def reset(self):           self.count = 0\n",
    "  def accumulate(self, learn):\n",
    "    bs = find_bs(learn.yb)\n",
    "    #print(self.total)\n",
    "    self.count += bs\n",
    "  @property\n",
    "  def value(self): return self.count if self.count != 0 else None\n",
    "  @property\n",
    "  def name(self):  return 'NumWindows' \n",
    "\n",
    "# Example that takes an average (while considering batch size `bs`)\n",
    "# %% ../nbs/13a_learner.ipynb 119\n",
    "class BinnedConfusionMatrixMetric(Metric):\n",
    "  \"\"\"Micro confusion matrix (enables computation of micro-versions of other metrics)\n",
    "     (i.e., will give total percentages over all batches)\n",
    "  \"\"\"\n",
    "  def __init__(self):  self.labels = [-1.0, 1.0, 0.0]\n",
    "  def reset(self):           self.total,self.count = 0.0,0\n",
    "  def accumulate(self, learn):\n",
    "    bs = find_bs(learn.yb)\n",
    "    # Scales by batch size (later divided by batch size)\n",
    "    self.total += learn.to_detach(self._metric_batch(learn.pred, *learn.yb)) * bs\n",
    "    #print(self.total)\n",
    "    self.count += bs\n",
    "  def _metric_batch(self, y_pred, y_true):\n",
    "    # Computes confusion matrix for this batch (Normalized to percentages)\n",
    "    # First convert to 1, 0, or -1\n",
    "    def bin_data(y):\n",
    "      near_zero_mask = y.abs().lt(1/255.0)\n",
    "      y = torch.sign(y)\n",
    "      y[near_zero_mask] = 0\n",
    "      return y\n",
    "    y_pred, y_true = bin_data(y_pred), bin_data(y_true)\n",
    "    labels = self.labels\n",
    "    # Sklearn confusion_matrix is far too slow, vectorized version below\n",
    "    cm = np.zeros((len(labels), len(labels)))\n",
    "    for i, lab_i in enumerate(labels):\n",
    "      for j, lab_j in enumerate(labels):\n",
    "        cm[i, j] = torch.logical_and(y_true == lab_i, y_pred == lab_j).float().mean().cpu().numpy()\n",
    "    return cm\n",
    "\n",
    "  @property\n",
    "  def value(self): return self.total/self.count if self.count != 0 else None\n",
    "  @property\n",
    "  def name(self):  return 'BinnedConfusionMatrix' \n",
    "  \n",
    "class BinnedNonZeroFalsePositiveRate(BinnedConfusionMatrixMetric):\n",
    "  \"\"\"Micro FPR where 0 is negative - (True zeros but predicted nonzero)/(Num true zeros)\"\"\"\n",
    "  @property\n",
    "  def value(self):\n",
    "    print('FPR calculation')\n",
    "    micro_cm = self.total / self.count\n",
    "    zero_idx = self.labels.index(0.0)\n",
    "    correctly_predicted_zeros = micro_cm[zero_idx, zero_idx] # TN\n",
    "    total_true_zeros = micro_cm[zero_idx, :].sum() # N\n",
    "    TNR = correctly_predicted_zeros/total_true_zeros # TN/N\n",
    "    # FPR = 1 - TNR\n",
    "    return correctly_predicted_zeros/total_true_zeros if self.count != 0 else None\n",
    "  @property\n",
    "  def name(self):  return 'MicroFPR' \n",
    "  \n",
    "class BinnedTruePositiveRate(BinnedConfusionMatrixMetric):\n",
    "  \"\"\"Micro FPR where 0 is negative - (True zeros but predicted nonzero)/(Num true zeros)\"\"\"\n",
    "  def __init__(self, positive_value):\n",
    "    super().__init__()\n",
    "    self.positive_value = positive_value\n",
    "  @property\n",
    "  def value(self):\n",
    "    micro_cm = self.total / self.count\n",
    "    positive_idx = self.labels.index(self.positive_value)\n",
    "    correctly_predicted = micro_cm[positive_idx, positive_idx] # TP\n",
    "    total_true = micro_cm[positive_idx, :].sum() # P\n",
    "    TPR = correctly_predicted/total_true # TP/N\n",
    "    return TPR if self.count != 0 else None\n",
    "  @property\n",
    "  def name(self):  return f'MicroTPR({self.positive_value:.0f})'\n",
    "  \n",
    "next_window_metrics = [\n",
    "  nn.MSELoss(), \n",
    "  BinnedNonZeroFalsePositiveRate(), \n",
    "  BinnedTruePositiveRate(1.0), \n",
    "  BinnedTruePositiveRate(-1.0)\n",
    "]\n",
    "next_window_metric_names = [m.name if hasattr(m, 'name') else 'MSE' for m in next_window_metrics]\n",
    "print(next_window_metrics)\n",
    "print(next_window_metric_names)\n",
    "\n",
    "# Segmentation metrics\n",
    "\n",
    "segmentation_metrics = [\n",
    "  foreground_acc,\n",
    "  DiceMulti(),\n",
    "]\n",
    "segmentation_metric_names = ['CE', 'UnitAcc', 'Dice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b07a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _TestSplitter(max_samples, seed=0):\n",
    "  \"Create function that splits `items` between train/val with `valid_pct` randomly.\"\n",
    "  def _inner(o):\n",
    "      if seed is not None: torch.manual_seed(seed)\n",
    "      rand_idx = L(list(torch.randperm(len(o)).numpy()))\n",
    "      cut = min(max_samples, len(o))\n",
    "      return rand_idx[cut:],rand_idx[:cut]\n",
    "  return _inner\n",
    "\n",
    "def test_unit_identification_model(name, batch_size, segment_path, path_to_model):\n",
    "    \n",
    "    FASTAI_ARCHES = dict(\n",
    "      unet_resnet18=resnet18,\n",
    "      unet_resnet34=resnet34,\n",
    "      unet_xresnet18=xresnet18_deep,\n",
    "      unet_xresnet34=xresnet34_deep,\n",
    "       unet_squeezenet1_0=squeezenet1_0,\n",
    "      unet_squeezenet1_1=squeezenet1_1,\n",
    "      unet_densenet121=densenet121,\n",
    "      unet_densenet169=densenet169,\n",
    "    )\n",
    "    \n",
    "    sc2_segment = DataBlock(\n",
    "                      blocks=(ImageBlock, MaskBlock(codes=CHANNEL_TO_NAME)),\n",
    "                      get_items=get_image_files,\n",
    "                      get_y=lambda filename: (os.path.splitext(filename)[0].replace('images','labels') + '_labels.png'),\n",
    "                      splitter=_TestSplitter(max_samples=args.max_samples, seed=0),\n",
    "                      batch_tfms=None)\n",
    "    dls = sc2_segment.dataloaders(segment_path/'images', shuffle=True, bs=batch_size, num_workers=12)\n",
    "    print(f'Datset at {segment_path}')\n",
    "    print(f'Test dataset has approximately {len(dls.valid)*batch_size} windows')\n",
    "\n",
    "    # Create learner\n",
    "    learner = unet_learner(arch=FASTAI_ARCHES[name], dls=dls)\n",
    "    print(f'Loss function for unit id: {learner.loss_func}')\n",
    "    learner.metrics = segmentation_metrics\n",
    "    learner = learner.load(path_to_model)    \n",
    "    \n",
    "    result = learner.validate()\n",
    "    # Now that testing has finished, empty the cache\n",
    "    torch.cuda.empty_cache()\n",
    "    return {m: r for r, m in zip(result, segmentation_metric_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df79b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Starting result aggregation on segmentation dataset: {str(segmentation_data_path)}. ')\n",
    "\n",
    "unit_identification_model_root = extracted_model_root / 'unit_identification'\n",
    "\n",
    "unit_identification_model_dirs = [f for f in unit_identification_model_root.glob('**/*') \n",
    "                                    if f.is_dir() and not str(f).startswith('unet_')]\n",
    "\n",
    "for model_dir in unit_identification_model_dirs[:min(args.max_models, len(unit_identification_model_dirs))]:\n",
    "    name = model_dir.name\n",
    "    arch = name.split('unet_')[-1]\n",
    "    \n",
    "    print('\\n'*5, f'Starting {name}.'.center(60, '-'))\n",
    "    \n",
    "    path_to_model = get_best_model(model_dir)\n",
    "    \n",
    "    print(f'Using model {path_to_model}')\n",
    "\n",
    "    result = test_unit_identification_model(name, args.batch_size, segmentation_data_path, path_to_model)\n",
    "    \n",
    "    \n",
    "    save_benchmark_results(benchmark_results_save_root, result, name, benchmark_kind='unit_identification',\n",
    "                             sensor_kwargs=None, metric_name='MSE', raw_model_path=path_to_model)\n",
    "    \n",
    "    print(f'Finished {name}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1026ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(benchmark_results_save_root/'unit_identification_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a77396",
   "metadata": {},
   "source": [
    "# Getting *sensor* unit type identification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33da46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_dataset = SegmentationDataset(segmentation_data_path,\n",
    "                                           create_metadata=True)\n",
    "print(f'Using path: {segmentation_data_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866be641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sensor_unit_identification_model(name, batch_size, segmentation_dataset,\n",
    "                                          path_to_model, sensor_kwargs):\n",
    "    \n",
    "    FASTAI_ARCHES = dict(\n",
    "      unet_resnet18=resnet18,\n",
    "      unet_resnet34=resnet34,\n",
    "      unet_xresnet18=xresnet18_deep,\n",
    "      unet_xresnet34=xresnet34_deep,\n",
    "       unet_squeezenet1_0=squeezenet1_0,\n",
    "      unet_squeezenet1_1=squeezenet1_1,\n",
    "      unet_densenet121=densenet121,\n",
    "      unet_densenet169=densenet169,\n",
    "    )\n",
    "    \n",
    "    segmentation_sensor_placement_dataset = SensorPlacementDataset(\n",
    "            segmentation_dataset, **sensor_kwargs,\n",
    "            return_mask=False, make_cache=False, noiseless_ground_truth=True)\n",
    "    \n",
    "    block = DataBlock(\n",
    "        blocks=(ImageBlock, MaskBlock),\n",
    "        get_items=lambda d: list(range(len(d))),\n",
    "        get_x=lambda idx: segmentation_sensor_placement_dataset[idx][0],\n",
    "        get_y=lambda idx: segmentation_sensor_placement_dataset[idx][1],\n",
    "        splitter=_TestSplitter(max_samples=args.max_samples, seed=0),\n",
    "    )\n",
    "    \n",
    "    dls = block.dataloaders(segmentation_sensor_placement_dataset, batch_size=batch_size)\n",
    "    print(f'Test dataset has {len(segmentation_sensor_placement_dataset)} windows and the valid dataloader has approximately {len(dls.valid)*batch_size} windows')\n",
    "\n",
    "    # Create learner\n",
    "    learner = unet_learner(arch=FASTAI_ARCHES[name], dls=dls, n_out=170)\n",
    "    learner.metrics = segmentation_metrics\n",
    "    learner = learner.load(path_to_model)    \n",
    "    \n",
    "    result = learner.validate()\n",
    "    # Now that testing has finished, empty the cache\n",
    "    torch.cuda.empty_cache()\n",
    "    return {m: r for r, m in zip(result, segmentation_metric_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6909502c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Starting **sensor** result aggregation on segmentation dataset: {str(segmentation_data_path)}. ')\n",
    "\n",
    "sensor_unit_identification_model_root = extracted_model_root / 'sensor_unit_identification'\n",
    "\n",
    "sensor_unit_identification_model_dirs = [f for f in sensor_unit_identification_model_root.glob('**/*') \n",
    "                                    if f.is_dir() and not str(f).endswith('_sensors')]\n",
    "\n",
    "for model_dir in sensor_unit_identification_model_dirs[:min(args.max_models, len(sensor_unit_identification_model_dirs))]:\n",
    "    sensor_kind = model_dir.parent.name.split('_sensors')[0]\n",
    "    name = model_dir.name\n",
    "    arch = name.split('unet_')[-1]\n",
    "    \n",
    "    sensor_kwargs = {'n_sensors': 50, 'radius': 5.5, 'kind': sensor_kind, 'failure_rate': 0.2}\n",
    "    \n",
    "    print('\\n'*5, f'Starting {name} with {sensor_kind} placement.'.center(60, '-'))\n",
    "    \n",
    "    path_to_model = get_best_model(model_dir)\n",
    "    \n",
    "    print(f'Using model {path_to_model}')\n",
    "\n",
    "    result = test_sensor_unit_identification_model(name, args.batch_size,\n",
    "                                                   segmentation_dataset, path_to_model, sensor_kwargs)\n",
    "    \n",
    "    \n",
    "    save_benchmark_results(benchmark_results_save_root, result, name, benchmark_kind='unit_identification',\n",
    "                             sensor_kwargs=sensor_kwargs, metric_name='MSE', raw_model_path=path_to_model)\n",
    "    \n",
    "    print(f'Finished {name} with {sensor_kind} placement.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585d4376",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.read_csv(benchmark_results_save_root/'sensor_unit_identification_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
