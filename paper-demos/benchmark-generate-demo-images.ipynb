{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "336d771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "\n",
    "from fastai.data.all import Transform, DataBlock, RandomSplitter, L\n",
    "from fastai.vision.all import *\n",
    "from fastai.distributed import *\n",
    "from fastai.callback.all import SaveModelCallback, EarlyStoppingCallback\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# Load StarCraft2Sensor stuff\n",
    "ipynb_dir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "code_root = os.path.join(ipynb_dir, '..')\n",
    "sys.path.append(code_root)  # Needed for import below\n",
    "\n",
    "from sc2sensor.dataset import StarCraftSensor\n",
    "from sc2sensor.utils.sensor_utils import SensorPlacementDataset, SUPPORTED_PLACEMENT_KINDS\n",
    "from sc2sensor.utils.unit_type_data import NONNEUTRAL_CHANNEL_TO_ID, NONNEUTRAL_ID_TO_NAME\n",
    "CHANNEL_TO_NAME = [NONNEUTRAL_ID_TO_NAME[NONNEUTRAL_CHANNEL_TO_ID[i]] for i in range(len(NONNEUTRAL_CHANNEL_TO_ID))]\n",
    "# removing barrier_h and barrier_v from the placement kinds since diag kinds are more fitting\n",
    "PLACEMENT_KINDS = [kind for kind in SUPPORTED_PLACEMENT_KINDS if not (kind.endswith('_h') or kind.endswith('_v'))]\n",
    "\n",
    "\n",
    "data_root = os.path.join(code_root, 'data') # Data root directory\n",
    "data_subdir = 'starcraft-sensor-dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2023649b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with inputs: {'max_samples': 60000, 'batch_size': 24, 'n_epochs': 10}\n",
      "Using a max number of a 60000\n"
     ]
    }
   ],
   "source": [
    "class FakeArgs():\n",
    "    def __str__(self):\n",
    "        return str(self.__dict__)\n",
    "\n",
    "args = FakeArgs()\n",
    "args.max_samples = 60000\n",
    "args.batch_size = 24\n",
    "args.n_epochs = 10\n",
    "\n",
    "print(f'Starting with inputs: {str(args)}')\n",
    "\n",
    "if args.max_samples is not None:\n",
    "    print(f'Using a max number of a {args.max_samples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51430acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_benchmark_results(save_root, result, name, benchmark_kind,\n",
    "                             sensor_kwargs=None, metric_name='MSE', raw_model_path=None):\n",
    "    if benchmark_kind == 'unit_identification':\n",
    "        results_file = 'unit_identification_results.csv' if sensor_kwargs is None else 'sensor_unit_identification_results.csv'\n",
    "    elif benchmark_kind == 'next_window':\n",
    "        results_file = 'next_window_results.csv' if sensor_kwargs is None else 'sensor_next_window_results.csv'\n",
    "    else:\n",
    "        raise NotImplementedError(f'No benchmark file exists for type {benchmark_kind}')\n",
    "        \n",
    "    save_root = Path(save_root)\n",
    "    if (save_root/results_file).exists():\n",
    "        current_results = pd.read_csv(save_root/results_file)\n",
    "    else:\n",
    "        current_results = pd.DataFrame()\n",
    "        \n",
    "    results_to_be_saved = {\n",
    "        'model_name':name,\n",
    "         metric_name: [result],\n",
    "        'sensor_kind':None if sensor_kwargs is None else sensor_kwargs['kind'],\n",
    "        'sensor_kwargs': [sensor_kwargs],\n",
    "        'raw_model_path':str(raw_model_path) if isinstance(raw_model_path, Path) else raw_model_path}\n",
    "    \n",
    "    results_df = pd.concat((current_results, pd.DataFrame(results_to_be_saved)))\n",
    "    results_df.to_csv(save_root/results_file, index=False)\n",
    "    return True\n",
    "\n",
    "def get_best_model(model_root, with_suffix=False):\n",
    "    model_root = Path(model_root)\n",
    "    train_history = pd.read_csv(model_root/'train_history.csv')\n",
    "    best_epoch = train_history.valid_loss.argmin()\n",
    "    best_model = [i for i in list(model_root.glob('*')) if str(i).endswith(f'_{best_epoch}.pth')]\n",
    "    assert len(best_model) == 1, f'Found {len(best_model)} best model(s) instead of just 1.'\n",
    "    if with_suffix:\n",
    "        return best_model[0]\n",
    "    else:\n",
    "        return best_model[0].with_suffix('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6f4053",
   "metadata": {},
   "source": [
    "# Next Window Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "659e73ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subclass of original dataset for next window prediction\n",
    "class NextWindowDataset(StarCraftSensor):\n",
    "  def __init__(self, *args, max_samples=None, **kwargs):\n",
    "    assert 'use_sparse' not in kwargs, 'Cannot set use_sparse with this dataset.'\n",
    "    assert 'compute_labels' not in kwargs, 'Cannot set use_sparse with this dataset.'\n",
    "    super().__init__(*args, use_sparse=True, compute_labels=False, **kwargs)\n",
    "    self.max_samples = max_samples\n",
    "    \n",
    "    # Sort data so that next index is merely + 1\n",
    "    self.metadata = self.metadata.sort_values(['static.replay_name', 'dynamic.window_idx']).reset_index(drop=True)\n",
    "    md = self.metadata\n",
    "    \n",
    "    # Get starting window indices\n",
    "    start_windows = md[(md['dynamic.num_windows'] > 1) \n",
    "                       & (md['dynamic.window_idx'] < (md['dynamic.num_windows'] - 1))]\n",
    "    self.start_idx = start_windows.index\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # Get original indices of start and end based on input \n",
    "    md = self.metadata\n",
    "    # Assumes sorted\n",
    "    orig_idx = self.start_idx[idx]\n",
    "    next_idx = orig_idx + 1\n",
    "    # Only sanity check first and last as this may be expensive\n",
    "    if idx == 0 or idx == len(self) - 1:\n",
    "      assert md['static.replay_name'][orig_idx] == md['static.replay_name'][next_idx], 'Replays are not the same'\n",
    "      assert md['dynamic.window_idx'][orig_idx] + 1 == md['dynamic.window_idx'][next_idx], 'Window indices are not adjacent'\n",
    "\n",
    "    # Get combined hyperspectral images\n",
    "    def get_hyperspectral_dense(idx):\n",
    "      # Concatenate player1 and player2 hyperspectral\n",
    "      replay_file, window_idx = self._get_replay_and_window_idx(idx)\n",
    "      with np.load(replay_file) as data:\n",
    "        player_1_hyperspectral = self._extract_hyperspectral(\n",
    "          'player_1', data, window_idx)\n",
    "        player_2_hyperspectral = self._extract_hyperspectral(\n",
    "          'player_2', data, window_idx)\n",
    "      return torch.concat([player_1_hyperspectral.to_dense(), \n",
    "                           player_2_hyperspectral.to_dense()], \n",
    "                          dim=-3).float()\n",
    "    windows = [get_hyperspectral_dense(idx) for idx in [orig_idx, next_idx]] \n",
    "    x = windows[0]\n",
    "    y = windows[1] - windows[0] # Compute diff\n",
    "    return x, y\n",
    "\n",
    "  def __len__(self):\n",
    "    if self.max_samples is not None:\n",
    "      return min(self.max_samples, len(self.start_idx))\n",
    "    else:\n",
    "      return len(self.start_idx)\n",
    "\n",
    "# Create fastai dataloaders given the PyTorch dataset\n",
    "class AddChannelCodes(Transform):\n",
    "  \"Add the code metadata to a `TensorMask`\"\n",
    "  def __init__(self, codes=None):\n",
    "      self.codes = codes\n",
    "      if codes is not None: self.vocab,self.c = codes,len(codes)\n",
    "\n",
    "  def decodes(self, o):\n",
    "      if self.codes is not None: o.codes=self.codes\n",
    "      return o\n",
    "    \n",
    "# HACK: Put all instances in both \"train\" and \"valid\"\n",
    "# From https://forums.fast.ai/t/solved-not-splitting-datablock/84759/3\n",
    "def all_splitter(o): return L(int(i) for i in range(len(o))), L(int(i) for i in range(len(o)))\n",
    "    \n",
    "SC2_CODES = [\n",
    "  f'{player}_{name}'\n",
    "  for player in ['P1','P2']\n",
    "  for name in CHANNEL_TO_NAME\n",
    "]\n",
    "def create_dataloaders_from_dataset(dataset, splitter=None, **kwargs):\n",
    "    # Needs to have reference to dataset for closures\n",
    "    assert 'get_x' not in kwargs\n",
    "    assert 'get_y' not in kwargs\n",
    "    assert 'get_items' not in kwargs\n",
    "    splitter = splitter if splitter is not None else RandomSplitter(seed=0)\n",
    "    assert len(SC2_CODES) == dataset[0][0].shape[0], 'Number of codes does not match number of channels'\n",
    "    block = DataBlock(\n",
    "        get_items=lambda d: list(range(len(d))),\n",
    "        get_x=lambda idx: dataset[idx][0],\n",
    "        get_y=lambda idx: dataset[idx][1],\n",
    "        blocks=None, # These are just transforms\n",
    "        splitter=splitter,\n",
    "        item_tfms=[AddChannelCodes(SC2_CODES)],\n",
    "    )\n",
    "    return block.dataloaders(dataset, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec9424f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached CSV metadata\n",
      "Not computing labels\n",
      "Post-processing metadata\n",
      "Finished dataset init\n",
      "Num Test: 60000\n",
      "x.shape: torch.Size([340, 64, 64]), y.shape: torch.Size([340, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Load test set\n",
    "next_window_test = NextWindowDataset(root=data_root, subdir=data_subdir,\n",
    "                                     train=False, max_samples=args.max_samples)\n",
    "\n",
    "print(f'Num Test: {len(next_window_test)}')\n",
    "print(f'x.shape: {next_window_test[-1][0].shape}, y.shape: {next_window_test[-1][1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7ea44f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_kwargs = {'n_sensors': 50,\n",
    "                 'radius': 5.5,\n",
    "                 'kind': 'grid',\n",
    "                 'failure_rate': 0.2}\n",
    "sensor_next_window_test = SensorPlacementDataset(next_window_test, **sensor_kwargs,\n",
    "                                                 return_mask=False, make_cache=False,\n",
    "                                                 noiseless_ground_truth=True)\n",
    "\n",
    "plt.imshow(sensor_next_window_test[0][0].sum(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32801352",
   "metadata": {},
   "source": [
    "# Unit Type Identification Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17edafb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sc2sensor.utils.unit_type_data import NONNEUTRAL_CHANNEL_TO_ID, NONNEUTRAL_ID_TO_NAME\n",
    "CHANNEL_TO_NAME = [NONNEUTRAL_ID_TO_NAME[NONNEUTRAL_CHANNEL_TO_ID[i]] for i in range(len(NONNEUTRAL_CHANNEL_TO_ID))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc491d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "class SegmentationDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, segment_path, create_metadata=True):\n",
    "        super().__init__()\n",
    "        self.path = Path(segment_path)\n",
    "        self.X_filenames = self._get_files()\n",
    "        if create_metadata:\n",
    "            self.metadata, self.match_metadata = self._make_metadata()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X_filename = self.X_filenames[idx]\n",
    "        y_filename = os.path.splitext(X_filename)[0].replace('images','labels') + '_labels.png'\n",
    "        \n",
    "#         return torch_read_image(str(X_filename)), torch_read_image(str(y_filename)).squeeze()\n",
    "        return (plt.imread(str(X_filename))*255).astype(np.uint8), \\\n",
    "               (plt.imread(str(y_filename))*255).astype(np.uint8)\n",
    "\n",
    "    def _get_files(self):\n",
    "        files =  list((self.path / 'images').glob('**/*.png'))\n",
    "        assert len(files) > 0, f'No .png files found in {self.path}'\n",
    "        return files\n",
    "    \n",
    "    def _make_metadata(self):\n",
    "        replay_names = [str(f).split('_')[-2].split('/')[-1] for f in self.X_filenames]\n",
    "        metadata = pd.DataFrame({'static.replay_name':replay_names})\n",
    "        match_metadata = metadata.drop_duplicates(subset=['static.replay_name']).reset_index(drop=True)\n",
    "        return metadata, match_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c3bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_data_path = Path(data_root)/'starcraft-sensor-dataset'/'segment'/'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2abc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_unit_identification_model(name, batch_size, segment_path, path_to_model):\n",
    "    \n",
    "    FASTAI_ARCHES = dict(\n",
    "      unet_resnet18=resnet18,\n",
    "      unet_resnet34=resnet34,\n",
    "      unet_xresnet18=xresnet18_deep,\n",
    "      unet_xresnet34=xresnet34_deep,\n",
    "       unet_squeezenet1_0=squeezenet1_0,\n",
    "      unet_squeezenet1_1=squeezenet1_1,\n",
    "      unet_densenet121=densenet121,\n",
    "      unet_densenet169=densenet169,\n",
    "    )\n",
    "    \n",
    "    sc2_segment = DataBlock(\n",
    "                      blocks=(ImageBlock, MaskBlock(codes=CHANNEL_TO_NAME)),\n",
    "                      get_items=get_image_files,\n",
    "                      get_y=lambda filename: (os.path.splitext(filename)[0].replace('images','labels') + '_labels.png'),\n",
    "                      splitter=RandomSplitter(seed=0),\n",
    "                      batch_tfms=None)\n",
    "    dls = sc2_segment.dataloaders(segment_path/'images', shuffle=True, bs=batch_size, num_workers=12)\n",
    "\n",
    "    # Create learner\n",
    "    learner = unet_learner(arch=FASTAI_ARCHES[name], dls=dls)\n",
    "    learner = learner.load(path_to_model)    \n",
    "    \n",
    "    result = learner.validate()\n",
    "    # Now that testing has finished, empty the cache\n",
    "    torch.cuda.empty_cache()\n",
    "    return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df79b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Starting result aggregation on segmentation dataset: {str(segmentation_data_path)}. ')\n",
    "\n",
    "unit_identification_model_root = extracted_model_root / 'unit_identification'\n",
    "\n",
    "unit_identification_model_dirs = [f for f in unit_identification_model_root.glob('**/*') \n",
    "                                    if f.is_dir() and not str(f).startswith('unet_')]\n",
    "\n",
    "for model_dir in unit_identification_model_dirs:\n",
    "    name = model_dir.name\n",
    "    arch = name.split('unet_')[-1]\n",
    "    \n",
    "    print('\\n'*5, f'Starting {name}.'.center(60, '-'))\n",
    "    \n",
    "    path_to_model = get_best_model(model_dir)\n",
    "    \n",
    "    print(f'Using model {path_to_model}')\n",
    "\n",
    "    result = test_unit_identification_model(name, args.batch_size, segmentation_data_path, path_to_model)\n",
    "    \n",
    "    \n",
    "    save_benchmark_results(benchmark_results_save_root, result, name, benchmark_kind='unit_identification',\n",
    "                             sensor_kwargs=None, metric_name='MSE', raw_model_path=path_to_model)\n",
    "    \n",
    "    print(f'Finished {name}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1026ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(benchmark_results_save_root/'unit_identification_results.csv').sort_values('MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a77396",
   "metadata": {},
   "source": [
    "# Getting *sensor* unit type identification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33da46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_dataset = SegmentationDataset(segmentation_data_path,\n",
    "                                           create_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866be641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sensor_unit_identification_model(name, batch_size, segmentation_dataset,\n",
    "                                          path_to_model, sensor_kwargs):\n",
    "    \n",
    "    FASTAI_ARCHES = dict(\n",
    "      unet_resnet18=resnet18,\n",
    "      unet_resnet34=resnet34,\n",
    "      unet_xresnet18=xresnet18_deep,\n",
    "      unet_xresnet34=xresnet34_deep,\n",
    "       unet_squeezenet1_0=squeezenet1_0,\n",
    "      unet_squeezenet1_1=squeezenet1_1,\n",
    "      unet_densenet121=densenet121,\n",
    "      unet_densenet169=densenet169,\n",
    "    )\n",
    "    \n",
    "    segmentation_sensor_placement_dataset = SensorPlacementDataset(\n",
    "            segmentation_dataset, **sensor_kwargs,\n",
    "            return_mask=False, make_cache=False, noiseless_ground_truth=True)\n",
    "    \n",
    "    block = DataBlock(\n",
    "        blocks=(ImageBlock, MaskBlock),\n",
    "        get_items=lambda d: list(range(len(d))),\n",
    "        get_x=lambda idx: segmentation_sensor_placement_dataset[idx][0],\n",
    "        get_y=lambda idx: segmentation_sensor_placement_dataset[idx][1],\n",
    "        splitter=RandomSplitter(seed=0),\n",
    "    )\n",
    "    \n",
    "    dls = block.dataloaders(segmentation_sensor_placement_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Create learner\n",
    "    learner = unet_learner(arch=FASTAI_ARCHES[name], dls=dls, n_out=170)\n",
    "    learner = learner.load(path_to_model)    \n",
    "    \n",
    "    result = learner.validate()\n",
    "    # Now that testing has finished, empty the cache\n",
    "    torch.cuda.empty_cache()\n",
    "    return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6909502c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Starting **sensor** result aggregation on segmentation dataset: {str(segmentation_data_path)}. ')\n",
    "\n",
    "sensor_unit_identification_model_root = extracted_model_root / 'sensor_unit_identification'\n",
    "\n",
    "sensor_unit_identification_model_dirs = [f for f in sensor_unit_identification_model_root.glob('**/*') \n",
    "                                    if f.is_dir() and not str(f).endswith('_sensors')]\n",
    "\n",
    "for model_dir in sensor_unit_identification_model_dirs:\n",
    "    sensor_kind = model_dir.parent.name.split('_sensors')[0]\n",
    "    name = model_dir.name\n",
    "    arch = name.split('unet_')[-1]\n",
    "    \n",
    "    sensor_kwargs = {'n_sensors': 50, 'radius': 5.5, 'kind': sensor_kind, 'failure_rate': 0.2}\n",
    "    \n",
    "    print('\\n'*5, f'Starting {name} with {sensor_kind} placement.'.center(60, '-'))\n",
    "    \n",
    "    path_to_model = get_best_model(model_dir)\n",
    "    \n",
    "    print(f'Using model {path_to_model}')\n",
    "\n",
    "    result = test_sensor_unit_identification_model(name, args.batch_size,\n",
    "                                                   segmentation_dataset, path_to_model, sensor_kwargs)\n",
    "    \n",
    "    \n",
    "    save_benchmark_results(benchmark_results_save_root, result, name, benchmark_kind='unit_identification',\n",
    "                             sensor_kwargs=sensor_kwargs, metric_name='MSE', raw_model_path=path_to_model)\n",
    "    \n",
    "    print(f'Finished {name} with {sensor_kind} placement.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585d4376",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.read_csv(benchmark_results_save_root/'sensor_unit_identification_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:starcraft] *",
   "language": "python",
   "name": "conda-env-starcraft-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
